
@article{lu_deepxde_2021,
	title = {{DeepXDE}: A Deep Learning Library for Solving Differential Equations},
	volume = {63},
	issn = {0036-1445},
	url = {https://epubs.siam.org/doi/10.1137/19M1274067},
	doi = {10.1137/19M1274067},
	shorttitle = {{DeepXDE}},
	abstract = {In this paper, we propose a personalized dialogue generation system, which combines reinforcement learning techniques with an attention-based hierarchical recurrent encoderdecoder model. Firstly, we incorporate user-specific information into the decoder to capture user's background information and speaking style. Secondly, we employ reinforcement learning techniques to maximize future reward in dialogue, which enables our system to generate topic-coherent, informative and grammatical responses. Moreover, we propose three types of rewards to characterize good conversations. Finally, we compare the performance of the following reinforcement learning methods in dialogue generation: policy gradient, Q-learning, and actor-critic algorithms. We conduct experiments to verify the effectiveness of the proposed model on two dialogue datasets. Experimental results demonstrate that our model can generate better personalized dialogues for different users. Quantitatively, our method achieves better performance than the state-of-the-art dialogue systems in terms of {BLEU} score, perplexity, and human evaluation.},
	pages = {208--228},
	number = {1},
	journaltitle = {{SIAM} Review},
	shortjournal = {{SIAM} Rev.},
	author = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
	urldate = {2023-03-15},
	date = {2021-01},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	file = {Full Text PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/3ZEMUITP/Lu et al. - 2021 - DeepXDE A Deep Learning Library for Solving Diffe.pdf:application/pdf},
}

@misc{araz_elvet_2021,
	title = {Elvet -- a neural network-based differential equation and variational problem solver},
	url = {http://arxiv.org/abs/2103.14575},
	doi = {10.48550/arXiv.2103.14575},
	abstract = {We present Elvet, a Python package for solving differential equations and variational problems using machine learning methods. Elvet can deal with any system of coupled ordinary or partial differential equations with arbitrary initial and boundary conditions. It can also minimize any functional that depends on a collection of functions of several variables while imposing constraints on them. The solution to any of these problems is represented as a neural network trained to produce the desired function.},
	number = {{arXiv}:2103.14575},
	publisher = {{arXiv}},
	author = {Araz, Jack Y. and Criado, Juan Carlos and Spannowsky, Michael},
	urldate = {2023-03-15},
	date = {2021-03-30},
	eprinttype = {arxiv},
	eprint = {2103.14575 [hep-lat, physics:hep-ph, physics:hep-th, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, High Energy Physics - Lattice, High Energy Physics - Phenomenology, High Energy Physics - Theory},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/IDFH4KM2/Araz et al. - 2021 - Elvet -- a neural network-based differential equat.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/8LQP88QI/2103.html:text/html},
}

@article{lee_neural_1990,
	title = {Neural algorithm for solving differential equations},
	volume = {91},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/002199919090007N},
	doi = {10.1016/0021-9991(90)90007-N},
	abstract = {Finite difference equations are considered to solve differential equations numerically by utilizing minimization algorithms. Neural minimization algorithms for solving the finite difference equations are presented. Results of numerical simulation are described to demonstrate the method. Methods of implementing the algorithms are discussed. General features of the neural algorithms are discussed.},
	pages = {110--131},
	number = {1},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Lee, Hyuk and Kang, In Seok},
	urldate = {2023-03-16},
	date = {1990-11-01},
	langid = {english},
	file = {ScienceDirect Full Text PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/SSCWVEGQ/Lee and Kang - 1990 - Neural algorithm for solving differential equation.pdf:application/pdf;ScienceDirect Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/AFTKLQVJ/002199919090007N.html:text/html},
}

@misc{han_solving_2018,
	title = {Solving high-dimensional partial differential equations using deep learning},
	url = {http://arxiv.org/abs/1707.02568},
	doi = {10.1073/pnas.1718942115},
	abstract = {Developing algorithms for solving high-dimensional partial differential equations ({PDEs}) has been an exceedingly difficult task for a long time, due to the notoriously difficult problem known as the "curse of dimensionality". This paper introduces a deep learning-based approach that can handle general high-dimensional parabolic {PDEs}. To this end, the {PDEs} are reformulated using backward stochastic differential equations and the gradient of the unknown solution is approximated by neural networks, very much in the spirit of deep reinforcement learning with the gradient acting as the policy function. Numerical results on examples including the nonlinear Black-Scholes equation, the Hamilton-Jacobi-Bellman equation, and the Allen-Cahn equation suggest that the proposed algorithm is quite effective in high dimensions, in terms of both accuracy and cost. This opens up new possibilities in economics, finance, operational research, and physics, by considering all participating agents, assets, resources, or particles together at the same time, instead of making ad hoc assumptions on their inter-relationships.},
	author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	urldate = {2023-03-16},
	date = {2018-07-03},
	eprinttype = {arxiv},
	eprint = {1707.02568 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability, Mathematics - Numerical Analysis, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/P2GG34RG/Han et al. - 2018 - Solving high-dimensional partial differential equa.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/3VQY3B9K/1707.html:text/html},
}

@misc{magill_neural_2018,
	title = {Neural Networks Trained to Solve Differential Equations Learn General Representations},
	url = {http://arxiv.org/abs/1807.00042},
	doi = {10.48550/arXiv.1807.00042},
	abstract = {We introduce a technique based on the singular vector canonical correlation analysis ({SVCCA}) for measuring the generality of neural network layers across a continuously-parametrized set of tasks. We illustrate this method by studying generality in neural networks trained to solve parametrized boundary value problems based on the Poisson partial differential equation. We find that the first hidden layer is general, and that deeper layers are successively more specific. Next, we validate our method against an existing technique that measures layer generality using transfer learning experiments. We find excellent agreement between the two methods, and note that our method is much faster, particularly for continuously-parametrized problems. Finally, we visualize the general representations of the first layers, and interpret them as generalized coordinates over the input domain.},
	number = {{arXiv}:1807.00042},
	publisher = {{arXiv}},
	author = {Magill, Martin and Qureshi, Faisal and de Haan, Hendrick W.},
	urldate = {2023-03-16},
	date = {2018-06-29},
	eprinttype = {arxiv},
	eprint = {1807.00042 [physics, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/WURRJGJU/Magill et al. - 2018 - Neural Networks Trained to Solve Differential Equa.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/IT2J42E3/1807.html:text/html},
}

@article{piscopo_solving_2019,
	title = {Solving differential equations with neural networks: Applications to the calculation of cosmological phase transitions},
	volume = {100},
	issn = {2470-0010, 2470-0029},
	url = {http://arxiv.org/abs/1902.05563},
	doi = {10.1103/PhysRevD.100.016002},
	shorttitle = {Solving differential equations with neural networks},
	abstract = {Starting from the observation that artificial neural networks are uniquely suited to solving optimisation problems, and most physics problems can be cast as an optimisation task, we introduce a novel way of finding a numerical solution to wide classes of differential equations. We find our approach to be very flexible and stable without relying on trial solutions, and applicable to ordinary, partial and coupled differential equations. We apply our method to the calculation of tunnelling profiles for cosmological phase transitions, which is a problem of relevance for baryogenesis and stochastic gravitational wave spectra. Comparing our solutions with publicly available codes which use numerical methods optimised for the calculation of tunnelling profiles, we find our approach to provide at least as accurate results as these dedicated differential equation solvers, and for some parameter choices even more accurate and reliable solutions. In particular, we compare the neural network approach with two publicly available profile solvers, {\textbackslash}texttt\{{CosmoTransitions}\} and {\textbackslash}texttt\{{BubbleProfiler}\}, and give explicit examples where the neural network approach finds the correct solution while dedicated solvers do not. We point out that this approach of using artificial neural networks to solve equations is viable for any problem that can be cast into the form \${\textbackslash}mathcal\{F\}({\textbackslash}vec\{x\})=0\$, and is thus applicable to various other problems in perturbative and non-perturbative quantum field theory.},
	pages = {016002},
	number = {1},
	journaltitle = {Physical Review D},
	shortjournal = {Phys. Rev. D},
	author = {Piscopo, Maria Laura and Spannowsky, Michael and Waite, Philip},
	urldate = {2023-03-16},
	date = {2019-07-09},
	eprinttype = {arxiv},
	eprint = {1902.05563 [hep-ph, physics:hep-th]},
	keywords = {High Energy Physics - Phenomenology, High Energy Physics - Theory},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/ATV622JP/Piscopo et al. - 2019 - Solving differential equations with neural network.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/XD6Q8TC6/1902.html:text/html},
}

@article{regazzoni_machine_2019,
	title = {Machine learning for fast and reliable solution of time-dependent differential equations},
	volume = {397},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999119305364},
	doi = {10.1016/j.jcp.2019.07.050},
	abstract = {We propose a data-driven Model Order Reduction ({MOR}) technique, based on Artificial Neural Networks ({ANNs}), applicable to dynamical systems arising from Ordinary Differential Equations ({ODEs}) or time-dependent Partial Differential Equations ({PDEs}). Unlike model-based approaches, the proposed approach is non-intrusive since it just requires a collection of input-output pairs generated through the high-fidelity ({HF}) {ODE} or {PDE} model. We formulate our model reduction problem as a maximum-likelihood problem, in which we look for the model that minimizes, in a class of candidate models, the error on the available input-output pairs. Specifically, we represent candidate models by means of {ANNs}, which we train to learn the dynamics of the {HF} model from the training input-output data. We prove that {ANN} models are able to approximate every time-dependent model described by {ODEs} with any desired level of accuracy. We test the proposed technique on different problems, including the model reduction of two large-scale models. Two of the {HF} systems of {ODEs} here considered stem from the spatial discretization of a parabolic and an hyperbolic {PDE} respectively, which sheds light on a promising field of application of the proposed technique.},
	pages = {108852},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Regazzoni, F. and Dedè, L. and Quarteroni, A.},
	urldate = {2023-03-16},
	date = {2019-11-15},
	langid = {english},
	keywords = {Machine learning, Artificial neural networks, Data-driven modeling, Differential equations, Model order reduction, System identification},
	file = {ScienceDirect Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/RKSVT43G/S0021999119305364.html:text/html;Submitted Version:/home/salih/snap/zotero-snap/common/Zotero/storage/A798HHF2/Regazzoni et al. - 2019 - Machine learning for fast and reliable solution of.pdf:application/pdf},
}

@misc{chen_neural_2019,
	title = {Neural Ordinary Differential Equations},
	url = {http://arxiv.org/abs/1806.07366},
	doi = {10.48550/arXiv.1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any {ODE} solver, without access to its internal operations. This allows end-to-end training of {ODEs} within larger models.},
	number = {{arXiv}:1806.07366},
	publisher = {{arXiv}},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	urldate = {2023-03-16},
	date = {2019-12-13},
	eprinttype = {arxiv},
	eprint = {1806.07366 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/YKSMKASH/Chen et al. - 2019 - Neural Ordinary Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/YMJT87AB/1806.html:text/html},
}

@misc{shen_deep_2020,
	title = {Deep Euler method: solving {ODEs} by approximating the local truncation error of the Euler method},
	url = {http://arxiv.org/abs/2003.09573},
	doi = {10.48550/arXiv.2003.09573},
	shorttitle = {Deep Euler method},
	abstract = {In this paper, we propose a deep learning-based method, deep Euler method ({DEM}) to solve ordinary differential equations. {DEM} significantly improves the accuracy of the Euler method by approximating the local truncation error with deep neural networks which could obtain a high precision solution with a large step size. The deep neural network in {DEM} is mesh-free during training and shows good generalization in unmeasured regions. {DEM} could be easily combined with other schemes of numerical methods, such as Runge-Kutta method to obtain better solutions. Furthermore, the error bound and stability of {DEM} is discussed.},
	number = {{arXiv}:2003.09573},
	publisher = {{arXiv}},
	author = {Shen, Xing and Cheng, Xiaoliang and Liang, Kewei},
	urldate = {2023-03-16},
	date = {2020-03-21},
	eprinttype = {arxiv},
	eprint = {2003.09573 [cs, math]},
	keywords = {Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/8EWEYBPS/Shen et al. - 2020 - Deep Euler method solving ODEs by approximating t.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/2C7HBM79/2003.html:text/html},
}

@article{rudd_constrained_2014,
	title = {A Constrained Backpropagation Approach for the Adaptive Solution of Partial Differential Equations},
	volume = {25},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2013.2277601},
	abstract = {This paper presents a constrained backpropagation ({CPROP}) methodology for solving nonlinear elliptic and parabolic partial differential equations ({PDEs}) adaptively, subject to changes in the {PDE} parameters or external forcing. Unlike existing methods based on penalty functions or Lagrange multipliers, {CPROP} solves the constrained optimization problem associated with training a neural network to approximate the {PDE} solution by means of direct elimination. As a result, {CPROP} reduces the dimensionality of the optimization problem, while satisfying the equality constraints associated with the boundary and initial conditions exactly, at every iteration of the algorithm. The effectiveness of this method is demonstrated through several examples, including nonlinear elliptic and parabolic {PDEs} with changing parameters and nonhomogeneous terms.},
	pages = {571--584},
	number = {3},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	author = {Rudd, Keith and Muro, Gianluca Di and Ferrari, Silvia},
	date = {2014-03},
	note = {Conference Name: {IEEE} Transactions on Neural Networks and Learning Systems},
	keywords = {Artificial neural networks, Adaptive algorithm, artificial neural networks ({ANNs}), Equations, Jacobian matrices, Linear programming, Optimization, partial differential equations ({PDEs}), scientific computing, Training},
	file = {IEEE Xplore Full Text PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/4XS7PIXG/Rudd et al. - 2014 - A Constrained Backpropagation Approach for the Ada.pdf:application/pdf},
}

@article{rudd_constrained_2015,
	title = {A constrained integration ({CINT}) approach to solving partial differential equations using artificial neural networks},
	volume = {155},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S092523121401652X},
	doi = {10.1016/j.neucom.2014.11.058},
	abstract = {This paper presents a novel constrained integration ({CINT}) method for solving initial boundary value partial differential equations ({PDEs}). The {CINT} method combines classical Galerkin methods with a constrained backpropogation training approach to obtain an artificial neural network representation of the {PDE} solution that approximately satisfies the boundary conditions at every integration step. The advantage of {CINT} over existing methods is that it is readily applicable to solving {PDEs} on irregular domains, and requires no special modification for domains with complex geometries. Furthermore, the {CINT} method provides a semi-analytical solution that is infinitely differentiable. In this paper the {CINT} method is demonstrated on two hyperbolic and one parabolic initial boundary value problems with a known analytical solutions that can be used for performance comparison. The numerical results show that, when compared to the most efficient finite element methods, the {CINT} method achieves significant improvements both in terms of computational time and accuracy.},
	pages = {277--285},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Rudd, Keith and Ferrari, Silvia},
	urldate = {2023-03-16},
	date = {2015-05-01},
	langid = {english},
	keywords = {Partial differential equations, Galerkin methods, Initial-boundary value problem, Irregular domains, Neural networks, Spectral methods},
	file = {ScienceDirect Full Text PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/VV3J5H24/Rudd and Ferrari - 2015 - A constrained integration (CINT) approach to solvi.pdf:application/pdf},
}

@misc{guidetti_dnnsolve_2021,
	title = {{dNNsolve}: an efficient {NN}-based {PDE} solver},
	url = {http://arxiv.org/abs/2103.08662},
	doi = {10.48550/arXiv.2103.08662},
	shorttitle = {{dNNsolve}},
	abstract = {Neural Networks ({NNs}) can be used to solve Ordinary and Partial Differential Equations ({ODEs} and {PDEs}) by redefining the question as an optimization problem. The objective function to be optimized is the sum of the squares of the {PDE} to be solved and of the initial/boundary conditions. A feed forward {NN} is trained to minimise this loss function evaluated on a set of collocation points sampled from the domain where the problem is defined. A compact and smooth solution, that only depends on the weights of the trained {NN}, is then obtained. This approach is often referred to as {PINN}, from Physics Informed Neural Network{\textasciitilde}{\textbackslash}cite\{raissi2017physics\_1, raissi2017physics\_2\}. Despite the success of the {PINN} approach in solving various classes of {PDEs}, an implementation of this idea that is capable of solving a large class of {ODEs} and {PDEs} with good accuracy and without the need to finely tune the hyperparameters of the network, is not available yet. In this paper, we introduce a new implementation of this concept - called {dNNsolve} - that makes use of dual Neural Networks to solve {ODEs}/{PDEs}. These include: i) sine and sigmoidal activation functions, that provide a more efficient basis to capture both secular and periodic patterns in the solutions; ii) a newly designed architecture, that makes it easy for the the {NN} to approximate the solution using the basis functions mentioned above. We show that {dNNsolve} is capable of solving a broad range of {ODEs}/{PDEs} in 1, 2 and 3 spacetime dimensions, without the need of hyperparameter fine-tuning.},
	number = {{arXiv}:2103.08662},
	publisher = {{arXiv}},
	author = {Guidetti, Veronica and Muia, Francesco and Welling, Yvette and Westphal, Alexander},
	urldate = {2023-03-16},
	date = {2021-03-15},
	eprinttype = {arxiv},
	eprint = {2103.08662 [astro-ph, physics:gr-qc, physics:hep-lat]},
	keywords = {Computer Science - Machine Learning, High Energy Physics - Lattice, Astrophysics - Astrophysics of Galaxies, Astrophysics - Cosmology and Nongalactic Astrophysics, General Relativity and Quantum Cosmology},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/AJGL2N9W/Guidetti et al. - 2021 - dNNsolve an efficient NN-based PDE solver.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/6VQP8522/2103.html:text/html},
}

@article{sirignano_dgm_2018,
	title = {{DGM}: A deep learning algorithm for solving partial differential equations},
	volume = {375},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118305527},
	doi = {10.1016/j.jcp.2018.08.029},
	shorttitle = {{DGM}},
	abstract = {High-dimensional {PDEs} have been a longstanding computational challenge. We propose to solve high-dimensional {PDEs} by approximating the solution with a deep neural network which is trained to satisfy the differential operator, initial condition, and boundary conditions. Our algorithm is meshfree, which is key since meshes become infeasible in higher dimensions. Instead of forming a mesh, the neural network is trained on batches of randomly sampled time and space points. The algorithm is tested on a class of high-dimensional free boundary {PDEs}, which we are able to accurately solve in up to 200 dimensions. The algorithm is also tested on a high-dimensional Hamilton–Jacobi–Bellman {PDE} and Burgers' equation. The deep learning algorithm approximates the general solution to the Burgers' equation for a continuum of different boundary conditions and physical conditions (which can be viewed as a high-dimensional space). We call the algorithm a “Deep Galerkin Method ({DGM})” since it is similar in spirit to Galerkin methods, with the solution approximated by a neural network instead of a linear combination of basis functions. In addition, we prove a theorem regarding the approximation power of neural networks for a class of quasilinear parabolic {PDEs}.},
	pages = {1339--1364},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	urldate = {2023-03-16},
	date = {2018-12-15},
	langid = {english},
	keywords = {Machine learning, Partial differential equations, Deep learning, High-dimensional partial differential equations},
	file = {ScienceDirect Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/SG8T92BH/S0021999118305527.html:text/html;Submitted Version:/home/salih/snap/zotero-snap/common/Zotero/storage/7NSHL9ZY/Sirignano and Spiliopoulos - 2018 - DGM A deep learning algorithm for solving partial.pdf:application/pdf},
}

@misc{koryagin_pydens_2019,
	title = {{PyDEns}: a Python Framework for Solving Differential Equations with Neural Networks},
	url = {http://arxiv.org/abs/1909.11544},
	doi = {10.48550/arXiv.1909.11544},
	shorttitle = {{PyDEns}},
	abstract = {Recently, a lot of papers proposed to use neural networks to approximately solve partial differential equations ({PDEs}). Yet, there has been a lack of flexible framework for convenient experimentation. In an attempt to fill the gap, we introduce a {PyDEns}-module open-sourced on {GitHub}. Coupled with capabilities of {BatchFlow}, open-source framework for convenient and reproducible deep learning, {PyDEns}-module allows to 1) solve partial differential equations from a large family, including heat equation and wave equation 2) easily search for the best neural-network architecture among the zoo, that includes {ResNet} and {DenseNet} 3) fully control the process of model-training by testing different point-sampling schemes. With that in mind, our main contribution goes as follows: implementation of a ready-to-use and open-source numerical solver of {PDEs} of a novel format, based on neural networks.},
	number = {{arXiv}:1909.11544},
	publisher = {{arXiv}},
	author = {Koryagin, Alexander and Khudorozkov, Roman and Tsimfer, Sergey},
	urldate = {2023-03-16},
	date = {2019-09-25},
	eprinttype = {arxiv},
	eprint = {1909.11544 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/GHQEL8Y3/Koryagin et al. - 2019 - PyDEns a Python Framework for Solving Differentia.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/WHLYIMS6/1909.html:text/html},
}

@misc{dockhorn_discussion_2019,
	title = {A Discussion on Solving Partial Differential Equations using Neural Networks},
	url = {http://arxiv.org/abs/1904.07200},
	doi = {10.48550/arXiv.1904.07200},
	abstract = {Can neural networks learn to solve partial differential equations ({PDEs})? We investigate this question for two (systems of) {PDEs}, namely, the Poisson equation and the steady Navier--Stokes equations. The contributions of this paper are five-fold. (1) Numerical experiments show that small neural networks ({\textless} 500 learnable parameters) are able to accurately learn complex solutions for systems of partial differential equations. (2) It investigates the influence of random weight initialization on the quality of the neural network approximate solution and demonstrates how one can take advantage of this non-determinism using ensemble learning. (3) It investigates the suitability of the loss function used in this work. (4) It studies the benefits and drawbacks of solving (systems of) {PDEs} with neural networks compared to classical numerical methods. (5) It proposes an exhaustive list of possible directions of future work.},
	number = {{arXiv}:1904.07200},
	publisher = {{arXiv}},
	author = {Dockhorn, Tim},
	urldate = {2023-03-16},
	date = {2019-04-15},
	eprinttype = {arxiv},
	eprint = {1904.07200 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/5848BIE5/Dockhorn - 2019 - A Discussion on Solving Partial Differential Equat.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/EUSQW5VZ/1904.html:text/html},
}

@article{avrutskiy_neural_2020,
	title = {Neural networks catching up with finite differences in solving partial differential equations in higher dimensions},
	volume = {32},
	issn = {0941-0643, 1433-3058},
	url = {http://arxiv.org/abs/1712.05067},
	doi = {10.1007/s00521-020-04743-8},
	abstract = {Fully connected multilayer perceptrons are used for obtaining numerical solutions of partial differential equations in various dimensions. Independent variables are fed into the input layer, and the output is considered as solution's value. To train such a network one can use square of equation's residual as a cost function and minimize it with respect to weights by gradient descent. Following previously developed method, derivatives of the equation's residual along random directions in space of independent variables are also added to cost function. Similar procedure is known to produce nearly machine precision results using less than 8 grid points per dimension for 2D case. The same effect is observed here for higher dimensions: solutions are obtained on low density grids, but maintain their precision in the entire region. Boundary value problems for linear and nonlinear Poisson equations are solved inside 2, 3, 4, and 5 dimensional balls. Grids for linear cases have 40, 159, 512 and 1536 points and for nonlinear 64, 350, 1536 and 6528 points respectively. In all cases maximum error is less than \$8.8{\textbackslash}cdot10{\textasciicircum}\{-6\}\$, and median error is less than \$2.4{\textbackslash}cdot10{\textasciicircum}\{-6\}\$. Very weak grid requirements enable neural networks to obtain solution of 5D linear problem within 22 minutes, whereas projected solving time for finite differences on the same hardware is 50 minutes. Method is applied to second order equation, but requires little to none modifications to solve systems or higher order {PDEs}.},
	pages = {13425--13440},
	number = {17},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Avrutskiy, V. I.},
	urldate = {2023-03-16},
	date = {2020-09},
	eprinttype = {arxiv},
	eprint = {1712.05067 [cs]},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/I52KW49R/Avrutskiy - 2020 - Neural networks catching up with finite difference.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/SF9V3KS9/1712.html:text/html},
}

@misc{sitzmann_implicit_2020,
	title = {Implicit Neural Representations with Periodic Activation Functions},
	url = {http://arxiv.org/abs/2006.09661},
	doi = {10.48550/arXiv.2006.09661},
	abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.},
	number = {{arXiv}:2006.09661},
	publisher = {{arXiv}},
	author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
	urldate = {2023-03-16},
	date = {2020-06-17},
	eprinttype = {arxiv},
	eprint = {2006.09661 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/DAZUNKJX/Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Acti.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/JKCTQ6KM/2006.html:text/html},
}

@inproceedings{silvescu_fourier_1999,
	title = {Fourier neural networks},
	volume = {1},
	doi = {10.1109/IJCNN.1999.831544},
	abstract = {A new kind of neuron model that has a Fourier-like in/out function is introduced. The model is discussed in a general theoretical framework and some completeness theorems are presented. Current experimental results show that the new model outperforms, by a large margin both in representational power and convergence speed, the classical mathematical model of neuron based on weighted sum of inputs filtered by a nonlinear function. The new model is also appealing from a neurophysiological point of view because it produces a more realistic representation by considering the inputs as oscillations.},
	eventtitle = {{IJCNN}'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},
	pages = {488--491 vol.1},
	booktitle = {{IJCNN}'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},
	author = {Silvescu, A.},
	date = {1999-07},
	note = {{ISSN}: 1098-7576},
	keywords = {Computer science, Artificial neural networks, Neural networks, Artificial intelligence, Computational modeling, Computer networks, Convergence, Intelligent networks, Mathematical model, Neurons},
	file = {IEEE Xplore Abstract Record:/home/salih/snap/zotero-snap/common/Zotero/storage/AS6KIXL8/authors.html:text/html},
}

@misc{abadi_tensorflow_2016,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Distributed Systems},
	url = {http://arxiv.org/abs/1603.04467},
	doi = {10.48550/arXiv.1603.04467},
	shorttitle = {{TensorFlow}},
	abstract = {{TensorFlow} is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using {TensorFlow} can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as {GPU} cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the {TensorFlow} interface and an implementation of that interface that we have built at Google. The {TensorFlow} {API} and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
	number = {{arXiv}:1603.04467},
	publisher = {{arXiv}},
	author = {Abadi, Martín and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
	urldate = {2023-03-16},
	date = {2016-03-16},
	eprinttype = {arxiv},
	eprint = {1603.04467 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/JBUBZYLW/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/8PN6L4FY/1603.html:text/html},
}

@article{sainio_cudaeasy_2010,
	title = {{CUDAEASY} - a {GPU} Accelerated Cosmological Lattice Program},
	volume = {181},
	issn = {00104655},
	url = {http://arxiv.org/abs/0911.5692},
	doi = {10.1016/j.cpc.2010.01.002},
	abstract = {This paper presents, to the author's knowledge, the first graphics processing unit ({GPU}) accelerated program that solves the evolution of interacting scalar fields in an expanding universe. We present the implementation in {NVIDIA}'s Compute Unified Device Architecture ({CUDA}) and compare the performance to other similar programs in chaotic inflation models. We report speedups between one and two orders of magnitude depending on the used hardware and software while achieving small errors in single precision. Simulations that used to last roughly one day to compute can now be done in hours and this difference is expected to increase in the future. The program has been written in the spirit of {LATTICEEASY} and users of the aforementioned program should find it relatively easy to start using {CUDAEASY} in lattice simulations. The program is available at http://www.physics.utu.fi/theory/particlecosmology/cudaeasy/ under the {GNU} General Public License.},
	pages = {906--912},
	number = {5},
	journaltitle = {Computer Physics Communications},
	shortjournal = {Computer Physics Communications},
	author = {Sainio, Jani},
	urldate = {2023-03-16},
	date = {2010-05},
	eprinttype = {arxiv},
	eprint = {0911.5692 [astro-ph, physics:hep-ph, physics:physics]},
	keywords = {Physics - Computational Physics, High Energy Physics - Phenomenology, Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/VGJN2EJX/Sainio - 2010 - CUDAEASY - a GPU Accelerated Cosmological Lattice .pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/TH8XQL2Y/0911.html:text/html},
}

@article{lagaris_artificial_1997,
	title = {Artificial Neural Network Methods in Quantum Mechanics},
	volume = {104},
	issn = {00104655},
	url = {http://arxiv.org/abs/quant-ph/9705029},
	doi = {10.1016/S0010-4655(97)00054-4},
	abstract = {In a previous article we have shown how one can employ Artificial Neural Networks ({ANNs}) in order to solve non-homogeneous ordinary and partial differential equations. In the present work we consider the solution of eigenvalue problems for differential and integrodifferential operators, using {ANNs}. We start by considering the Schr{\textbackslash}"odinger equation for the Morse potential that has an analytically known solution, to test the accuracy of the method. We then proceed with the Schr{\textbackslash}"odinger and the Dirac equations for a muonic atom, as well as with a non-local Schr{\textbackslash}"odinger integrodifferential equation that models the \$n+{\textbackslash}alpha\$ system in the framework of the resonating group method. In two dimensions we consider the well studied Henon-Heiles Hamiltonian and in three dimensions the model problem of three coupled anharmonic oscillators. The method in all of the treated cases proved to be highly accurate, robust and efficient. Hence it is a promising tool for tackling problems of higher complexity and dimensionality.},
	pages = {1--14},
	number = {1},
	journaltitle = {Computer Physics Communications},
	shortjournal = {Computer Physics Communications},
	author = {Lagaris, I. E. and Likas, A. and Fotiadis, D. I.},
	urldate = {2023-03-16},
	date = {1997-08},
	eprinttype = {arxiv},
	eprint = {quant-ph/9705029},
	keywords = {Physics - Computational Physics, Nonlinear Sciences - Cellular Automata and Lattice Gases, Quantum Physics},
}

@article{lagaris_artificial_1998,
	title = {Artificial neural networks for solving ordinary and partial differential equations},
	volume = {9},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/712178/},
	doi = {10.1109/72.712178},
	abstract = {We present a method to solve initial and boundary value problems using artiﬁcial neural networks. A trial solution of the differential equation is written as a sum of two parts. The ﬁrst part satisﬁes the initial/boundary conditions and contains no adjustable parameters. The second part is constructed so as not to affect the initial/boundary conditions. This part involves a feedforward neural network containing adjustable parameters (the weights). Hence by construction the initial/boundary conditions are satisﬁed and the network is trained to satisfy the differential equation. The applicability of this approach ranges from single ordinary differential equations ({ODE}’s), to systems of coupled {ODE}’s and also to partial differential equations ({PDE}’s). In this article, we illustrate the method by solving a variety of model problems and present comparisons with solutions obtained using the Galekrkin ﬁnite element method for several cases of partial differential equations. With the advent of neuroprocessors and digital signal processors the method becomes particularly interesting due to the expected essential gains in the execution speed.},
	pages = {987--1000},
	number = {5},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	shortjournal = {{IEEE} Trans. Neural Netw.},
	author = {Lagaris, I.E. and Likas, A. and Fotiadis, D.I.},
	urldate = {2023-03-16},
	date = {1998-09},
	langid = {english},
	file = {Lagaris et al. - 1998 - Artificial neural networks for solving ordinary an.pdf:/home/salih/snap/zotero-snap/common/Zotero/storage/IQZHLB43/Lagaris et al. - 1998 - Artificial neural networks for solving ordinary an.pdf:application/pdf},
}

@article{lagaris_neural-network_2000,
	title = {Neural-network methods for boundary value problems with irregular boundaries},
	volume = {11},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/870037/},
	doi = {10.1109/72.870037},
	abstract = {Partial differential equations ({PDEs}) with boundary conditions (Dirichlet or Neumann) defined on boundaries with simple geometry have been successfully treated using sigmoidal multilayer perceptrons in previous works. This article deals with the case of complex boundary geometry, where the boundary is determined by a number of points that belong to it and are closely located, so as to offer a reasonable representation. Two networks are employed: a multilayer perceptron and a radial basis function network. The later is used to account for the exact satisfaction of the boundary conditions. The method has been successfully tested on two-dimensional and three-dimensional {PDEs} and has yielded accurate results.},
	pages = {1041--1049},
	number = {5},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	shortjournal = {{IEEE} Trans. Neural Netw.},
	author = {Lagaris, I.E. and Likas, A.C. and Papageorgiou, D.G.},
	urldate = {2023-03-16},
	date = {2000-09},
	langid = {english},
	file = {Lagaris et al. - 2000 - Neural-network methods for boundary value problems.pdf:/home/salih/snap/zotero-snap/common/Zotero/storage/DE239X9C/Lagaris et al. - 2000 - Neural-network methods for boundary value problems.pdf:application/pdf},
}

@misc{racca_automatic-differentiated_2021,
	title = {Automatic-differentiated Physics-Informed Echo State Network ({API}-{ESN})},
	url = {http://arxiv.org/abs/2101.00002},
	doi = {10.48550/arXiv.2101.00002},
	abstract = {We propose the Automatic-differentiated Physics-Informed Echo State Network ({API}-{ESN}). The network is constrained by the physical equations through the reservoir's exact time-derivative, which is computed by automatic differentiation. As compared to the original Physics-Informed Echo State Network, the accuracy of the time-derivative is increased by up to seven orders of magnitude. This increased accuracy is key in chaotic dynamical systems, where errors grows exponentially in time. The network is showcased in the reconstruction of unmeasured (hidden) states of a chaotic system. The {API}-{ESN} eliminates a source of error, which is present in existing physics-informed echo state networks, in the computation of the time-derivative. This opens up new possibilities for an accurate reconstruction of chaotic dynamical states.},
	number = {{arXiv}:2101.00002},
	publisher = {{arXiv}},
	author = {Racca, Alberto and Magri, Luca},
	urldate = {2023-03-16},
	date = {2021-03-24},
	eprinttype = {arxiv},
	eprint = {2101.00002 [nlin]},
	keywords = {Computer Science - Machine Learning, Nonlinear Sciences - Chaotic Dynamics},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/LGWXKT69/Racca and Magri - 2021 - Automatic-differentiated Physics-Informed Echo Sta.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/2FX2QSDP/2101.html:text/html},
}

@misc{hennigh_nvidia_2020,
	title = {{NVIDIA} {SimNet}{\textasciicircum}\{{TM}\}: an {AI}-accelerated multi-physics simulation framework},
	url = {http://arxiv.org/abs/2012.07938},
	doi = {10.48550/arXiv.2012.07938},
	shorttitle = {{NVIDIA} {SimNet}{\textasciicircum}\{{TM}\}},
	abstract = {We present {SimNet}, an {AI}-driven multi-physics simulation framework, to accelerate simulations across a wide range of disciplines in science and engineering. Compared to traditional numerical solvers, {SimNet} addresses a wide range of use cases - coupled forward simulations without any training data, inverse and data assimilation problems. {SimNet} offers fast turnaround time by enabling parameterized system representation that solves for multiple configurations simultaneously, as opposed to the traditional solvers that solve for one configuration at a time. {SimNet} is integrated with parameterized constructive solid geometry as well as {STL} modules to generate point clouds. Furthermore, it is customizable with {APIs} that enable user extensions to geometry, physics and network architecture. It has advanced network architectures that are optimized for high-performance {GPU} computing, and offers scalable performance for multi-{GPU} and multi-Node implementation with accelerated linear algebra as well as {FP}32, {FP}64 and {TF}32 computations. In this paper we review the neural network solver methodology, the {SimNet} architecture, and the various features that are needed for effective solution of the {PDEs}. We present real-world use cases that range from challenging forward multi-physics simulations with turbulence and complex 3D geometries, to industrial design optimization and inverse problems that are not addressed efficiently by the traditional solvers. Extensive comparisons of {SimNet} results with open source and commercial solvers show good correlation.},
	number = {{arXiv}:2012.07938},
	publisher = {{arXiv}},
	author = {Hennigh, Oliver and Narasimhan, Susheela and Nabian, Mohammad Amin and Subramaniam, Akshay and Tangsali, Kaustubh and Rietmann, Max and Ferrandis, Jose del Aguila and Byeon, Wonmin and Fang, Zhiwei and Choudhry, Sanjay},
	urldate = {2023-03-16},
	date = {2020-12-14},
	eprinttype = {arxiv},
	eprint = {2012.07938 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Fluid Dynamics},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/4F2SJ64I/Hennigh et al. - 2020 - NVIDIA SimNet^ TM an AI-accelerated multi-physic.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/NCEYGP8B/2012.html:text/html},
}

@online{noauthor_passing_nodate,
	title = {Passing tf.keras.Model as tf.function argument does not create concrete function · Issue \#38875 · tensorflow/tensorflow},
	url = {https://github.com/tensorflow/tensorflow/issues/38875},
	abstract = {System information Have I written custom code: Yes {TensorFlow} installed from (source or binary): binary {TensorFlow} version (use command below): 2.1.0 Python version: 3.7.5 Describe the current beha...},
	titleaddon = {{GitHub}},
	urldate = {2023-03-16},
	langid = {english},
	file = {Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/G5B98ZHS/38875.html:text/html},
}

@misc{klambauer_self-normalizing_2017,
	title = {Self-Normalizing Neural Networks},
	url = {http://arxiv.org/abs/1706.02515},
	doi = {10.48550/arXiv.1706.02515},
	abstract = {Deep Learning has revolutionized vision via convolutional neural networks ({CNNs}) and natural language processing via recurrent neural networks ({RNNs}). However, success stories of Deep Learning with standard feed-forward neural networks ({FNNs}) are rare. {FNNs} that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks ({SNNs}) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of {SNNs} automatically converge towards zero mean and unit variance. The activation function of {SNNs} are "scaled exponential linear units" ({SELUs}), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of {SNNs} allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared {SNNs} on (a) 121 tasks from the {UCI} machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard {FNNs} and other machine learning methods such as random forests and support vector machines. {SNNs} significantly outperformed all competing {FNN} methods at 121 {UCI} tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning {SNN} architectures are often very deep. Implementations are available at: github.com/bioinf-jku/{SNNs}.},
	number = {{arXiv}:1706.02515},
	publisher = {{arXiv}},
	author = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
	urldate = {2023-03-17},
	date = {2017-09-07},
	eprinttype = {arxiv},
	eprint = {1706.02515 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/YKEL4VK7/Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/WJTS9CYL/1706.html:text/html},
}

@misc{betancourt_geometric_2018,
	title = {A Geometric Theory of Higher-Order Automatic Differentiation},
	url = {http://arxiv.org/abs/1812.11592},
	doi = {10.48550/arXiv.1812.11592},
	abstract = {First-order automatic differentiation is a ubiquitous tool across statistics, machine learning, and computer science. Higher-order implementations of automatic differentiation, however, have yet to realize the same utility. In this paper I derive a comprehensive, differential geometric treatment of automatic differentiation that naturally identifies the higher-order differential operators amenable to automatic differentiation as well as explicit procedures that provide a scaffolding for high-performance implementations.},
	number = {{arXiv}:1812.11592},
	publisher = {{arXiv}},
	author = {Betancourt, Michael},
	urldate = {2023-03-18},
	date = {2018-12-30},
	eprinttype = {arxiv},
	eprint = {1812.11592 [stat]},
	keywords = {Statistics - Computation},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/3WZ3JDHL/Betancourt - 2018 - A Geometric Theory of Higher-Order Automatic Diffe.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/DYCP9MU2/1812.html:text/html},
}

@inproceedings{bettencourt_taylor-mode_2022,
	title = {Taylor-Mode Automatic Differentiation for Higher-Order Derivatives in {JAX}},
	url = {https://openreview.net/forum?id=SkxEF3FNPH},
	abstract = {One way to achieve higher-order automatic differentiation ({AD}) is to implement first-order {AD} and apply it repeatedly. This nested approach works, but can result in combinatorial amounts of redundant work. This paper describes a more efficient method, already known but with a new presentation, and its implementation in {JAX}. We also study its application to neural ordinary differential equations, and in particular discuss some additional algorithmic improvements for higher-order {AD} of differential equations.},
	eventtitle = {Program Transformations for {ML} Workshop at {NeurIPS} 2019},
	author = {Bettencourt, Jesse and Johnson, Matthew J. and Duvenaud, David},
	urldate = {2023-03-18},
	date = {2022-07-05},
	langid = {english},
	file = {Full Text PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/P9E3P9FH/Bettencourt et al. - 2022 - Taylor-Mode Automatic Differentiation for Higher-O.pdf:application/pdf},
}

@article{margenberg_neural_2022,
	title = {A neural network multigrid solver for the Navier-Stokes equations},
	volume = {460},
	issn = {00219991},
	url = {http://arxiv.org/abs/2008.11520},
	doi = {10.1016/j.jcp.2022.110983},
	abstract = {We present the deep neural network multigrid solver ({DNN}-{MG}) that we develop for the instationary Navier-Stokes equations. {DNN}-{MG} improves computational efficiency using a judicious combination of a geometric multigrid solver and a recurrent neural network with memory. {DNN}-{MG} uses the multi-grid method to classically solve on coarse levels while the neural network corrects interpolated solutions on fine ones, thus avoiding the increasingly expensive computations that would have to be performed there. This results in a reduction in computation time through {DNN}-{MG}'s highly compact neural network. The compactness results from its design for local patches and the available coarse multigrid solutions that provides a "guide" for the corrections. A compact neural network with a small number of parameters also reduces training time and data. Furthermore, the network's locality facilitates generalizability and allows one to use {DNN}-{MG} trained on one mesh domain also on different ones. We demonstrate the efficacy of {DNN}-{MG} for variations of the 2D laminar flow around an obstacle. For these, our method significantly improves the solutions as well as lift and drag functionals while requiring only about half the computation time of a full multigrid solution. We also show that {DNN}-{MG} trained for the configuration with one obstacle can be generalized to other time dependent problems that can be solved efficiently using a geometric multigrid method.},
	pages = {110983},
	journaltitle = {Journal of Computational Physics},
	shortjournal = {Journal of Computational Physics},
	author = {Margenberg, Nils and Hartmann, Dirk and Lessig, Christian and Richter, Thomas},
	urldate = {2023-03-18},
	date = {2022-07},
	eprinttype = {arxiv},
	eprint = {2008.11520 [physics]},
	keywords = {Physics - Computational Physics, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/H4S623A7/Margenberg et al. - 2022 - A neural network multigrid solver for the Navier-S.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/FWMAA8UJ/2008.html:text/html},
}

@article{rao_physics-informed_2020,
	title = {Physics-informed deep learning for incompressible laminar flows},
	volume = {10},
	issn = {20950349},
	url = {http://arxiv.org/abs/2002.10558},
	doi = {10.1016/j.taml.2020.01.039},
	abstract = {Physics-informed deep learning has drawn tremendous interest in recent years to solve computational physics problems, whose basic concept is to embed physical laws to constrain/inform neural networks, with the need of less data for training a reliable model. This can be achieved by incorporating the residual of physics equations into the loss function. Through minimizing the loss function, the network could approximate the solution. In this paper, we propose a mixed-variable scheme of physics-informed neural network ({PINN}) for fluid dynamics and apply it to simulate steady and transient laminar flows at low Reynolds numbers. A parametric study indicates that the mixed-variable scheme can improve the {PINN} trainability and the solution accuracy. The predicted velocity and pressure fields by the proposed {PINN} approach are also compared with the reference numerical solutions. Simulation results demonstrate great potential of the proposed {PINN} for fluid flow simulation with a high accuracy.},
	pages = {207--212},
	number = {3},
	journaltitle = {Theoretical and Applied Mechanics Letters},
	shortjournal = {Theoretical and Applied Mechanics Letters},
	author = {Rao, Chengping and Sun, Hao and Liu, Yang},
	urldate = {2023-03-20},
	date = {2020-03},
	eprinttype = {arxiv},
	eprint = {2002.10558 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Physics - Fluid Dynamics},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/UCYJP58J/Rao et al. - 2020 - Physics-informed deep learning for incompressible .pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/F42VXJMG/2002.html:text/html},
}

@misc{peng_idrlnet_2021,
	title = {{IDRLnet}: A Physics-Informed Neural Network Library},
	url = {http://arxiv.org/abs/2107.04320},
	doi = {10.48550/arXiv.2107.04320},
	shorttitle = {{IDRLnet}},
	abstract = {Physics Informed Neural Network ({PINN}) is a scientific computing framework used to solve both forward and inverse problems modeled by Partial Differential Equations ({PDEs}). This paper introduces {IDRLnet}, a Python toolbox for modeling and solving problems through {PINN} systematically. {IDRLnet} constructs the framework for a wide range of {PINN} algorithms and applications. It provides a structured way to incorporate geometric objects, data sources, artificial neural networks, loss metrics, and optimizers within Python. Furthermore, it provides functionality to solve noisy inverse problems, variational minimization, and integral differential equations. New {PINN} variants can be integrated into the framework easily. Source code, tutorials, and documentation are available at {\textbackslash}url\{https://github.com/idrl-lab/idrlnet\}.},
	number = {{arXiv}:2107.04320},
	publisher = {{arXiv}},
	author = {Peng, Wei and Zhang, Jun and Zhou, Weien and Zhao, Xiaoyu and Yao, Wen and Chen, Xiaoqian},
	urldate = {2023-03-20},
	date = {2021-07-09},
	eprinttype = {arxiv},
	eprint = {2107.04320 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/T5Q4HG56/Peng et al. - 2021 - IDRLnet A Physics-Informed Neural Network Library.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/BWHAIXQM/2107.html:text/html},
}

@misc{mcclenny_tensordiffeq_2021,
	title = {{TensorDiffEq}: Scalable Multi-{GPU} Forward and Inverse Solvers for Physics Informed Neural Networks},
	url = {http://arxiv.org/abs/2103.16034},
	doi = {10.48550/arXiv.2103.16034},
	shorttitle = {{TensorDiffEq}},
	abstract = {Physics-Informed Neural Networks promise to revolutionize science and engineering practice, by introducing domain-aware deep machine learning models into scientific computation. Several software suites have emerged to make the implementation and usage of these architectures available to the research and industry communities. Here we introduce{\textbackslash}linebreak {TensorDiffEq}, built on Tensorflow 2.x, which presents an intuitive Keras-like interface for problem domain definition, model definition, and solution of forward and inverse problems using physics-aware deep learning methods. {TensorDiffEq} takes full advantage of Tensorflow 2.x infrastructure for deployment on multiple {GPUs}, allowing the implementation of large high-dimensional and complex models. Simultaneously, {TensorDiffEq} supports the Keras {API} for custom neural network architecture definitions. In the case of smaller or simpler models, the package allows for rapid deployment on smaller-scale {CPU} platforms with negligible changes to the implementation scripts. We demonstrate the basic usage and capabilities of {TensorDiffEq} in solving forward, inverse, and data assimilation problems of varying sizes and levels of complexity. The source code is available at https://github.com/tensordiffeq.},
	number = {{arXiv}:2103.16034},
	publisher = {{arXiv}},
	author = {{McClenny}, Levi D. and Haile, Mulugeta A. and Braga-Neto, Ulisses M.},
	urldate = {2023-03-20},
	date = {2021-03-29},
	eprinttype = {arxiv},
	eprint = {2103.16034 [physics]},
	note = {version: 1},
	keywords = {Computer Science - Mathematical Software, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/home/salih/snap/zotero-snap/common/Zotero/storage/XH3DJF9Z/McClenny et al. - 2021 - TensorDiffEq Scalable Multi-GPU Forward and Inver.pdf:application/pdf;arXiv.org Snapshot:/home/salih/snap/zotero-snap/common/Zotero/storage/QMRRGU4G/2103.html:text/html},
}
